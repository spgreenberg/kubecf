# This configuration tree specifies the default resource limits and requests
# for the kubecf instance groups and jobs.
#
# Syntax: See `chart/values.schema.yaml`
#
# NOTE: Missing instance groups and/or jobs are filled using data from
#  	the jobs tree.
#
#	Missing instance groups are added with `'$defaults': 2048`.
#       This is a very generous global limit. Stricter limits must be
#       specified here.
#
#	Instance groups without '$defaults' are extended with `'$defaults': 2048`.
#	Missing jobs are added with nil value -> This invokes the '$defaults'
#
#	This is all handled by `_resources.update` in `chart/templates/_resources.tpl`.
#
# NOTE 2: We need process names per job per group. This information is taken
#         from the data in `chart/config/jobs.yaml`, the per-job `processes` key.
#
#         All the processes of a job are given the same limit.
#
# NOTE 3: The `as*` groupds are for autoscaler. As they do not exist
# 	  in cf-deployment the auto-fill will not work for them. Thus
# 	  them being listed in full.
#
# The information from here is used by the helm templating creating
# ops instructions from the resource configuration for jobs to determine
# which jobs to ignore in the current deployment, in a generic
# manner. I.e not mixed into the templating code itself.
#
# All relevant supporting and using files are
#
# - chart/config/jobs.yaml			Job conditions
# - chart/config/resources.yaml			Self
# - chart/operations/resource-limits.yaml	Transform resource config to ops
# - chart/templates/_jobs.tpl			Take conditions and complete/resolve
# - chart/templates/_resources.tpl		Take self and complete
# - chart/templates/bosh_deployment.yaml	Reference the new config map
# - chart/templates/ops.yaml			Wrap `resources-limits.yaml` into config map
# - chart/values.schema.yaml			Job, resource config structure spec

# # ## ### ###### ######## ############# #####################

# resources:
#   '$defaults':
#     memory:
#       limit: 32
#   instance_group:
#     job:
#       process:
#         memory:
#           limit: 32
#           request: 16
#      job2: 128

resources:
  '$defaults': 32
  asactors:
    '$defaults': 30
    operator: ~
    scalingengine: ~
    scheduler: 800
  asapi:
    '$defaults': 30
    golangapiserver: ~
    metricsforwarder: ~
    route_registrar: ~
  asdatabase:
    '$defaults': 30
    postgres: 300
  asmetrics:
    '$defaults': 30
    eventgenerator: ~
    metricsserver: ~
  asnozzle:
    '$defaults': 40
    metricsgateway: ~

  api:
    '$defaults': 550
    cloud_controller_ng:
      cloud_controller_ng: 2048
      local_worker_1: {memory: {limit: 550, request: 400}}
      local_worker_2: 550
  cc-worker:
    '$defaults': 350
  credhub:
    '$defaults': 15
    credhub:
      credhub:
        memory:
          limit: 750
  diego-api:
    '$defaults': 50
  diego-cell:
    '$defaults': 40
    garden: 2048
    rep: 3072
  doppler:
    '$defaults': 40
    doppler: 70
  log-api:
    '$defaults': 100
  log-cache:
    '$defaults': 40
    log-cache: 2048
  nats:
    '$defaults': 30
  router:
    '$defaults': 100
  scheduler:
    '$defaults': 40
    cc_deployment_updater: 260
    cloud_controller_clock: 360
  singleton-blobstore:
    '$defaults': 330
  tcp-router:
    '$defaults': 60
  uaa:
    '$defaults': 30
    uaa: 1100
