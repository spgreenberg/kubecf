# This configuration tree specifies the default resource limits and requests
# for the kubecf instance groups and jobs.
#
# Syntax: See `chart/values.schema.yaml`
#
# NOTE: Missing instance groups and/or jobs are filled using data from
#  	the jobs tree.
#
#	Missing instance groups are added with `'$defaults': 2048`.
#       This is a very generous global limit. Stricter limits must be
#       specified here.
#
#	Instance groups without '$defaults' are extended with `'$defaults': 2048`.
#	Missing jobs are added with nil value -> This invokes the '$defaults'
#
#	This is all handled by `_resources.update` in `chart/templates/_resources.tpl`.
#
# NOTE 2: We need process names per job per group. This information is taken
#         from the data in `chart/config/jobs.yaml`, the per-job `processes` key.
#
#         All the processes of a job are given the same limit.
#
# NOTE 3: The `as*` groupds are for autoscaler. As they do not exist
# 	  in cf-deployment the auto-fill will not work for them. Thus
# 	  them being listed in full.
#
# The information from here is used by the helm templating creating
# ops instructions from the resource configuration for jobs to determine
# which jobs to ignore in the current deployment, in a generic
# manner. I.e not mixed into the templating code itself.
#
# All relevant supporting and using files are
#
# - chart/config/jobs.yaml			Job conditions
# - chart/config/resources.yaml			Self
# - chart/operations/resource-limits.yaml	Transform resource config to ops
# - chart/templates/_jobs.tpl			Take conditions and complete/resolve
# - chart/templates/_resources.tpl		Take self and complete
# - chart/templates/bosh_deployment.yaml	Reference the new config map
# - chart/templates/ops.yaml			Wrap `resources-limits.yaml` into config map
# - chart/values.schema.yaml			Job, resource config structure spec

# # ## ### ###### ######## ############# #####################

# non-advertised feature because we don't have any default cpu settings yet
features:
  cpu_limits:
    enabled: true

# resources:
#   '$defaults':
#     memory:
#       limit: 32
#   instance_group:
#     job:
#       process:
#         memory:
#           limit: 32
#           request: 16
#      job2: 128

addon_resources:
  '$defaults': 32

resources:
  '$defaults': 32

  asactors:
    scheduler: 800
  asapi:
  asdatabase:
    postgres: 300
  asmetrics:
  asnozzle:
    metricsgateway: 40

  api:
    cloud_controller_ng:
      cloud_controller_ng: 2048
      nginx: 192
      # Template for all `local_worker_N` processes.
      # Will be replaced by _jobs.update.
      '$local_worker': {memory: {limit: 550, request: 400}}
    file_server: 96
  cc-worker:
    cloud_controller_worker:
      # Template for all `worker_N` processes.
      # Will be replaced by _jobs.update.
      '$worker': 350
  credhub: 1024
  diego-api:
    '$defaults': 64
  diego-cell:
    '$defaults': 64
    garden:
      garden: {memory: {limit: 2048, request: 2048}}
    rep:
      rep: {memory: {limit: 3072, request: 2048}}
  doppler:
    doppler: 128
  log-api:
    '$defaults': 128
  log-cache:
    '$defaults': 40
    log-cache: 2048
  nats:
  router: 100
  scheduler:
    cc_deployment_updater: 320
    cloud_controller_clock: 512
  singleton-blobstore:
    blobstore:
      nginx: 400
  tcp-router: 96
  uaa:
    uaa: 1400
