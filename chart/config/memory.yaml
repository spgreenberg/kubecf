# This configuration tree specifies the default memory limits for the
# kubecf instance groups and jobs.
#
# Syntax: See `chart/values.schema.yaml`
#
# NOTE: Missing instance groups and/or jobs are filled using data from
#  	the jobs tree.
#
#	Missing instance groups are added with `'$default': 2Gi`.
#       This is a very generous global limit. Stricter limits must be
#       specified here.
#
#	Instance groups without '$default' are extended with `'$default': 2Gi`.
#	Missing jobs are added with nil value -> This invokes the '$default'
#
#	This is all handled by `_memory.update` in `chart/templates/_memory.tpl`.
#
# NOTE 2: We need process names per job per group. This information is taken
#         from the data in `chart/config/jobs.yaml`, the per-job `processes` key.
#
#         All the processes of a job are given the same limit.
#
# NOTE 3: The `as*` groupds are for autoscaler. As they do not exist
# 	  in cf-deployment the auto-fill will not work for them. Thus
# 	  them being listed in full.
#
# The information from here is used by the helm templating creating
# ops instructions from the memory configuration for jobs to determine
# which jobs to ignore in the current deployment, in a generic
# manner. I.e not mixed into the templating code itself.
#
# All relevant supporting and using files are
#
# - chart/config/jobs.yaml			Job conditions
# - chart/config/memory.yaml			Self
# - chart/operations/memory-limits.yaml		Transform memory config to ops
# - chart/templates/_jobs.tpl			Take conditions and complete/resolve
# - chart/templates/_memory.tpl			Take self and complete
# - chart/templates/bosh_deployment.yaml	Reference the new config map
# - chart/templates/ops.yaml			Wrap `memory-limits.yaml` into config map
# - chart/values.schema.yaml			Job, memory config structure spec

# # ## ### ###### ######## ############# #####################

limits:
  memory:
    asactors:
      '$default': 30Mi
      operator: ~
      scalingengine: ~
      scheduler: 800Mi
    asapi:
      '$default': 30Mi
      golangapiserver: ~
      metricsforwarder: ~
      route_registrar: ~
    asdatabase:
      '$default': 30Mi
      postgres: 300Mi
    asmetrics:
      '$default': 30Mi
      eventgenerator: ~
      metricsserver: ~
    asnozzle:
      '$default': 40Mi
      metricsgateway: ~

    api:
      '$default': 550Mi
      cloud_controller_ng: 2Gi
    cc-worker:
      '$default': 350Mi
    credhub:
      '$default': 15Mi
      credhub: 750Mi
    diego-api:
      '$default': 50Mi
    diego-cell:
      '$default': 40Mi
      garden: 2Gi
      rep: 3Gi
    doppler:
      '$default': 40Mi
      doppler: 70Mi
    log-api:
      '$default': 100Mi
    log-cache:
      '$default': 40Mi
      log-cache: 26Gi
    nats:
      '$default': 30Mi
    router:
      '$default': 100Mi
    scheduler:
      '$default': 40Mi
      cc_deployment_updater: 260Mi
      cloud_controller_clock: 360Mi
    singleton-blobstore:
      '$default': 330Mi
    tcp-router:
      '$default': 60Mi
    uaa:
      '$default': 30Mi
      uaa: 1100Mi
